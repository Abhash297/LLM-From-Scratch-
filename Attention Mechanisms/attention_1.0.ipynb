{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb54fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aaf245",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6da212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded866d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcb35e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5412706",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Attention Mechanisms from Scratch\n",
    "\n",
    "This notebook explores attention mechanisms step by step, starting with basic embeddings and building up to self-attention.\n",
    "\n",
    "## Starting with Word Embeddings\n",
    "\n",
    "Consider the following input sentence, which has already been embedded into 3-dimensional vectors. We choose a small embedding dimension for illustration purposes to ensure it fits on the page without line breaks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192292ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings shape: torch.Size([6, 3])\n",
      "Input embeddings:\n",
      "tensor([[0.4300, 0.1500, 0.8900],\n",
      "        [0.5500, 0.8700, 0.6600],\n",
      "        [0.5700, 0.8500, 0.6400],\n",
      "        [0.2200, 0.5800, 0.3300],\n",
      "        [0.7700, 0.2500, 0.1000],\n",
      "        [0.0500, 0.8000, 0.5500]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your      (x^1)\n",
    "     [0.55, 0.87, 0.66], # journey   (x^2)\n",
    "     [0.57, 0.85, 0.64], # starts    (x^3)\n",
    "     [0.22, 0.58, 0.33], # with      (x^4)\n",
    "     [0.77, 0.25, 0.10], # one       (x^5)\n",
    "     [0.05, 0.80, 0.55]] # step      (x^6)\n",
    ")\n",
    "\n",
    "print(\"Input embeddings shape:\", inputs.shape)\n",
    "print(\"Input embeddings:\")\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Computing Attention for a Single Query\n",
    "\n",
    "Let's compute the attention for the word \"journey\" (the second token, x^2). This will show us how much attention \"journey\" pays to each word in the sequence, including itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02720748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query token (journey): tensor([0.5500, 0.8700, 0.6600])\n",
      "\n",
      "Raw attention scores: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "\n",
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum of weights: tensor(1.)\n",
      "\n",
      "Context vector for 'journey': tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # Select \"journey\" (x^2) as our query\n",
    "print(\"Query token (journey):\", query)\n",
    "print()\n",
    "\n",
    "# Compute attention scores by taking dot product of query with all inputs\n",
    "attention_scores = torch.matmul(query, inputs.T)\n",
    "print(\"Raw attention scores:\", attention_scores)\n",
    "print()\n",
    "\n",
    "# Apply softmax to get attention weights (they sum to 1)\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "print(\"Attention weights:\", attention_weights)\n",
    "print(\"Sum of weights:\", attention_weights.sum())\n",
    "print()\n",
    "\n",
    "# Create context vector as weighted combination of all input vectors\n",
    "context_vector = torch.matmul(attention_weights, inputs)\n",
    "print(\"Context vector for 'journey':\", context_vector)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### What does this context vector represent?\n",
    "\n",
    "The context vector is a **weighted combination** of all input vectors, where the weights are determined by how much attention the query token (\"journey\") pays to each token in the sequence.\n",
    "\n",
    "- If \"journey\" pays high attention to \"Your\", then \"Your\"'s embedding contributes more to the final context vector\n",
    "- If \"journey\" pays low attention to \"step\", then \"step\"'s embedding contributes less\n",
    "- The attention weights always sum to 1, making this a weighted average\n",
    "\n",
    "This context vector now represents \"journey\" **in the context** of the entire sentence, incorporating information from all other words based on their relevance.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38376c5c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Computing Context Vectors for All Words\n",
    "\n",
    "Now let's compute the context vectors for all words in the sentence simultaneously. This is the full self-attention mechanism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e79921ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores matrix (6x6):\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "\n",
      "Attention weights matrix (each row sums to 1):\n",
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "\n",
      "Row sums (should all be 1.0):\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "\n",
      "Context vectors for all words:\n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n",
      "\n",
      "Shape: torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "# Compute attention scores for all queries at once\n",
    "# inputs @ inputs.T gives us a 6x6 matrix where each row is attention scores for one query\n",
    "attention_scores_all = torch.matmul(inputs, inputs.T)\n",
    "print(\"Attention scores matrix (6x6):\")\n",
    "print(attention_scores_all)\n",
    "print()\n",
    "\n",
    "# Apply softmax to each row to get attention weights\n",
    "attention_weights_all = torch.softmax(attention_scores_all, dim=-1)\n",
    "print(\"Attention weights matrix (each row sums to 1):\")\n",
    "print(attention_weights_all)\n",
    "print()\n",
    "\n",
    "# Verify that each row sums to 1\n",
    "print(\"Row sums (should all be 1.0):\")\n",
    "print(attention_weights_all.sum(dim=-1))\n",
    "print()\n",
    "\n",
    "# Compute context vectors for all words\n",
    "context_vectors_all = torch.matmul(attention_weights_all, inputs)\n",
    "print(\"Context vectors for all words:\")\n",
    "print(context_vectors_all)\n",
    "print()\n",
    "print(\"Shape:\", context_vectors_all.shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22148824",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Understanding the Results\n",
    "\n",
    "**Attention Scores Matrix (6x6):**\n",
    "- Each row `i` contains the raw attention scores for word `i` attending to all words\n",
    "- Higher values mean stronger attention\n",
    "\n",
    "**Attention Weights Matrix (6x6):**\n",
    "- Same as above but after softmax normalization\n",
    "- Each row sums to 1, making it a probability distribution\n",
    "- Row `i` shows how much word `i` attends to each word in the sequence\n",
    "\n",
    "**Context Vectors Matrix (6x3):**\n",
    "- Each row `i` is the context vector for word `i`\n",
    "- This is the contextualized representation of each word\n",
    "- Row 1 (index 1) should match our earlier single context vector for \"journey\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520dded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that our single context vector for \"journey\" matches the matrix result\n",
    "print(\"Single context vector for 'journey' (computed earlier):\")\n",
    "print(context_vector)\n",
    "print()\n",
    "print(\"Context vector for 'journey' from matrix (row 1):\")\n",
    "print(context_vectors_all[1])\n",
    "print()\n",
    "print(\"Are they the same?\", torch.allclose(context_vector, context_vectors_all[1]))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf578c65",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b225888",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#implementing STANDARD self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7af9506c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3367,  0.1288,  0.2345],\n",
      "        [ 0.2303, -1.1229, -0.1863],\n",
      "        [ 2.2082, -0.6380,  0.4617]])\n",
      "tensor([[ 0.2674,  0.5349,  0.8094],\n",
      "        [ 1.1103, -1.6898, -0.9890],\n",
      "        [ 0.9580,  1.3221,  0.8172]])\n",
      "tensor([[-0.7658, -0.7506,  1.3525],\n",
      "        [ 0.6863, -0.3278,  0.7950],\n",
      "        [ 0.2815,  0.0562,  0.5227]])\n"
     ]
    }
   ],
   "source": [
    "# Randomly initializing the weight matrices\n",
    "torch.manual_seed(42)\n",
    "\n",
    "W_q = torch.randn(3, 3)\n",
    "W_k = torch.randn(3, 3)\n",
    "W_v = torch.randn(3, 3)\n",
    "\n",
    "print (W_q)\n",
    "print (W_k)\n",
    "print (W_v)\n",
    "\n",
    "# Computing the query, key, and value vectors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74e726f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input token 'journey': tensor([0.5500, 0.8700, 0.6600])\n",
      "Input dimension (d_in): 3\n",
      "Output dimension (d_out): 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set dimensions like in the book\n",
    "x_2 = inputs[1]  # Focus on \"journey\" token first\n",
    "d_in = inputs.shape[1]  # Input dimension = 3\n",
    "d_out = 2  # Output dimension (smaller for illustration)\n",
    "\n",
    "print(f\"Input token 'journey': {x_2}\")\n",
    "print(f\"Input dimension (d_in): {d_in}\")\n",
    "print(f\"Output dimension (d_out): {d_out}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Comparison: Simple vs Standard Attention\n",
    "\n",
    "# **Key Differences:**\n",
    "\n",
    "# 1. **Simple Self-Attention**: Direct dot product of embeddings\n",
    "#    - `attention = softmax(inputs @ inputs.T)`\n",
    "#    - Uses original 3D embeddings\n",
    "#    - Context vector is 3D\n",
    "\n",
    "# 2. **Standard QKV Attention**: Learned transformations first\n",
    "#    - `attention = softmax(Q @ K.T / âˆšd_k)`\n",
    "#    - Uses transformed 2D vectors (Q, K, V)\n",
    "#    - Context vector is 2D\n",
    "#    - Has learnable parameters (W_q, W_k, W_v)\n",
    "#    - Includes scaling factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd629a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_query matrix:\n",
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]])\n",
      "\n",
      "W_key matrix:\n",
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]])\n",
      "\n",
      "W_value matrix:\n",
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the three weight matrices W_q, W_k, and W_v\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "\n",
    "print(\"W_query matrix:\")\n",
    "print(W_query)\n",
    "print(\"\\nW_key matrix:\")\n",
    "print(W_key)\n",
    "print(\"\\nW_value matrix:\")\n",
    "print(W_value)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2ef30037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query vector for 'journey': tensor([0.4306, 1.4551])\n",
      "Key vector for 'journey': tensor([0.4433, 1.1419])\n",
      "Value vector for 'journey': tensor([0.3951, 1.0037])\n",
      "\n",
      "COMPARISON:\n",
      "Original embedding for 'journey': tensor([0.5500, 0.8700, 0.6600])\n",
      "Query (what 'journey' is looking for): tensor([0.4306, 1.4551])\n",
      "Key (what 'journey' advertises about itself): tensor([0.4433, 1.1419])\n",
      "Value (information 'journey' provides): tensor([0.3951, 1.0037])\n"
     ]
    }
   ],
   "source": [
    "# Compute query, key, and value vectors for token \"journey\"\n",
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key  \n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(\"Query vector for 'journey':\", query_2)\n",
    "print(\"Key vector for 'journey':\", key_2)\n",
    "print(\"Value vector for 'journey':\", value_2)\n",
    "print()\n",
    "\n",
    "# Show the difference between simple and QKV attention\n",
    "print(\"COMPARISON:\")\n",
    "print(\"Original embedding for 'journey':\", x_2)\n",
    "print(\"Query (what 'journey' is looking for):\", query_2)\n",
    "print(\"Key (what 'journey' advertises about itself):\", key_2)\n",
    "print(\"Value (information 'journey' provides):\", value_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "224421b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8855, 0.9941, 0.3705],\n",
      "        [0.5148, 0.2103, 0.9562],\n",
      "        [0.6591, 0.4172, 0.6253]])\n",
      "tensor([[0.9961, 0.7036, 0.7429],\n",
      "        [0.9616, 0.5214, 0.5024],\n",
      "        [0.6241, 0.0379, 0.6748]])\n",
      "tensor([[0.4962, 0.4761, 0.5288],\n",
      "        [0.9429, 0.6435, 0.0470],\n",
      "        [0.9632, 0.8049, 0.7523]])\n",
      "\n",
      " query vector for the word journey\n",
      "tensor([1.3699, 1.0051, 1.4483])\n",
      "\n",
      " key vector for the word journey\n",
      "tensor([1.7964, 0.8656, 1.2910])\n",
      "\n",
      " value vector for the word journey\n",
      "tensor([1.7290, 1.3529, 0.8283])\n",
      "\n",
      " attention weights for the word journey\n",
      "tensor([0.0659, 0.3977, 0.3876, 0.0360, 0.0567, 0.0561])\n",
      "\n",
      " context vector for the word journey\n",
      "tensor([1.5756, 1.2382, 0.7768])\n"
     ]
    }
   ],
   "source": [
    "# First lets see how the context vector is computed for a single word\n",
    "\n",
    "x_2 = inputs[1] # word = journey\n",
    "d_in = inputs.shape[1] # dimension of the input vectors's column\n",
    "d_out = 3 # dimension of the output vectors\n",
    "\n",
    "W_q = torch.rand(d_in, d_out)\n",
    "W_k = torch.rand(d_in, d_out)\n",
    "W_v = torch.rand(d_in, d_out)\n",
    "\n",
    "print (W_q)\n",
    "print (W_k)\n",
    "print (W_v)\n",
    "\n",
    "q = x_2 @ W_q # query vector for the word \"journey\"\n",
    "k = x_2 @ W_k # key vector for the word \"journey\"\n",
    "v = x_2 @ W_v # value vector for the word \"journey\"\n",
    "\n",
    "#The following K and V are used to compute the attention weights for all words in the sentence\n",
    "#needed  as we will now use it with journey word to compute the attention weights for all words \n",
    "#in the sentence in relation to journey word\n",
    "Q = inputs @ W_q # query vector for all words in the sentence\n",
    "K = inputs @ W_k # key vector for all words in the sentence\n",
    "V = inputs @ W_v # value vector for all words in the sentence\n",
    "\n",
    "print (\"\\n query vector for the word journey\")\n",
    "print (q)\n",
    "print (\"\\n key vector for the word journey\")\n",
    "print (k)\n",
    "print (\"\\n value vector for the word journey\")\n",
    "print (v)\n",
    "\n",
    "\n",
    "attention_scores = torch.matmul(q, K.T) # calculating the attention scores for all words in the sentence in relation to journey word\n",
    "#q contains the query vector for the word \"journey\"\n",
    "#K contains the key vector for all words in the sentence\n",
    "#the result of the dot product of q and K.T is the attention scores for all words in the sentence in relation to journey word   \n",
    "\n",
    "\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1) # this is the attention weights for all words in the sentence in relation to journey word\n",
    "#attention_scores is the dot product of q and K.T\n",
    "#softmax is used to convert the attention scores into attention weights\n",
    "#the attention weights are then used to compute the context vector for the word \"journey\"   \n",
    "print (\"\\n attention weights for the word journey\")\n",
    "print (attention_weights)\n",
    "\n",
    "context_vector = torch.matmul(attention_weights, V) # this is the context vector for the word \"journey\"\n",
    "#attention_weights is the attention weights for all words in the sentence in relation to journey word\n",
    "#V contains the value vector for all words in the sentence\n",
    "#the result of the dot product of attention_weights and V is the context vector for the word \"journey\"  \n",
    "# context_vector contains the context vector for the word \"journey\"\n",
    "# it is a new representation of the word \"journey\" that captures the context of the sentence\n",
    "print (\"\\n context vector for the word journey\")\n",
    "print (context_vector)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (\"\\n attention scores for the word journey \")\n",
    "# print (attention_scores)\n",
    "# print (\"\\n attention weights shape\")\n",
    "# print (attention_weights.shape)\n",
    "# print (\"\\n value shape\")\n",
    "# print (v.shape)\n",
    "    \n",
    "# context_vector = torch.matmul(attention_weights, v)\n",
    "\n",
    "# print (\"\\n context vector for the word journey\")\n",
    "# print (context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d7507bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Context vectors for all words:\n",
      "[tensor([1.4076, 1.1098, 0.7100]), tensor([1.4618, 1.1514, 0.7327]), tensor([1.4605, 1.1505, 0.7322]), tensor([1.3737, 1.0839, 0.6971]), tensor([1.3835, 1.0912, 0.6995]), tensor([1.3940, 1.0997, 0.7063])]\n"
     ]
    }
   ],
   "source": [
    "# caclulating context vector for all words in the sentence\n",
    "# making a loop to calculate the context vector for all words in the sentence\n",
    "context_vectors = []\n",
    "\n",
    "Q = inputs @ W_q # query vector for all words in the sentence\n",
    "K = inputs @ W_k # key vector for all words in the sentence\n",
    "V = inputs @ W_v # value vector for all words in the sentence\n",
    "\n",
    "\n",
    "for i in range(inputs.shape[0]):\n",
    "    q = Q[i]  # query vector for word \n",
    "    attention_scores = torch.matmul(q, K.T)  #calculating the attention scores for all words in the sentence in relation to the word i\n",
    "    attention_weights = torch.softmax(attention_scores / (K.shape[-1] ** 0.5), dim=-1)  # scaled\n",
    "    context_vector = torch.matmul(attention_weights, V) #calculating the context vector for the word i in relation to all words in the sentence\n",
    "\n",
    "    context_vectors.append(context_vector)\n",
    "\n",
    "\n",
    "print(\"\\nContext vectors for all words:\")\n",
    "print(context_vectors) # this is the context vector for all words in the sentence in relation to all words in the sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad650882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

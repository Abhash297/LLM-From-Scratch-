{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Attention Mechanisms from Scratch\n",
    "\n",
    "This notebook explores attention mechanisms step by step, starting with basic embeddings and building up to self-attention.\n",
    "\n",
    "## Starting with Word Embeddings\n",
    "\n",
    "Consider the following input sentence, which has already been embedded into 3-dimensional vectors. We choose a small embedding dimension for illustration purposes to ensure it fits on the page without line breaks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192292ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input embeddings shape: torch.Size([6, 3])\n",
      "Input embeddings:\n",
      "tensor([[0.4300, 0.1500, 0.8900],\n",
      "        [0.5500, 0.8700, 0.6600],\n",
      "        [0.5700, 0.8500, 0.6400],\n",
      "        [0.2200, 0.5800, 0.3300],\n",
      "        [0.7700, 0.2500, 0.1000],\n",
      "        [0.0500, 0.8000, 0.5500]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], # Your      (x^1)\n",
    "     [0.55, 0.87, 0.66], # journey   (x^2)\n",
    "     [0.57, 0.85, 0.64], # starts    (x^3)\n",
    "     [0.22, 0.58, 0.33], # with      (x^4)\n",
    "     [0.77, 0.25, 0.10], # one       (x^5)\n",
    "     [0.05, 0.80, 0.55]] # step      (x^6)\n",
    ")\n",
    "\n",
    "print(\"Input embeddings shape:\", inputs.shape)\n",
    "print(\"Input embeddings:\")\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Computing Attention for a Single Query\n",
    "\n",
    "Let's compute the attention for the word \"journey\" (the second token, x^2). This will show us how much attention \"journey\" pays to each word in the sequence, including itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02720748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query token (journey): tensor([0.5500, 0.8700, 0.6600])\n",
      "\n",
      "Raw attention scores: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "\n",
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum of weights: tensor(1.)\n",
      "\n",
      "Context vector for 'journey': tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1]  # Select \"journey\" (x^2) as our query\n",
    "print(\"Query token (journey):\", query)\n",
    "print()\n",
    "\n",
    "# Compute attention scores by taking dot product of query with all inputs\n",
    "attention_scores = torch.matmul(query, inputs.T)\n",
    "print(\"Raw attention scores:\", attention_scores)\n",
    "print()\n",
    "\n",
    "# Apply softmax to get attention weights (they sum to 1)\n",
    "attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "print(\"Attention weights:\", attention_weights)\n",
    "print(\"Sum of weights:\", attention_weights.sum())\n",
    "print()\n",
    "\n",
    "# Create context vector as weighted combination of all input vectors\n",
    "context_vector = torch.matmul(attention_weights, inputs)\n",
    "print(\"Context vector for 'journey':\", context_vector)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### What does this context vector represent?\n",
    "\n",
    "The context vector is a **weighted combination** of all input vectors, where the weights are determined by how much attention the query token (\"journey\") pays to each token in the sequence.\n",
    "\n",
    "- If \"journey\" pays high attention to \"Your\", then \"Your\"'s embedding contributes more to the final context vector\n",
    "- If \"journey\" pays low attention to \"step\", then \"step\"'s embedding contributes less\n",
    "- The attention weights always sum to 1, making this a weighted average\n",
    "\n",
    "This context vector now represents \"journey\" **in the context** of the entire sentence, incorporating information from all other words based on their relevance.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "38376c5c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Computing Context Vectors for All Words\n",
    "\n",
    "Now let's compute the context vectors for all words in the sentence simultaneously. This is the full self-attention mechanism.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e79921ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores matrix (6x6):\n",
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "\n",
      "Attention weights matrix (each row sums to 1):\n",
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "\n",
      "Row sums (should all be 1.0):\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n",
      "\n",
      "Context vectors for all words:\n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n",
      "\n",
      "Shape: torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "# Compute attention scores for all queries at once\n",
    "# inputs @ inputs.T gives us a 6x6 matrix where each row is attention scores for one query\n",
    "attention_scores_all = torch.matmul(inputs, inputs.T)\n",
    "print(\"Attention scores matrix (6x6):\")\n",
    "print(attention_scores_all)\n",
    "print()\n",
    "\n",
    "# Apply softmax to each row to get attention weights\n",
    "attention_weights_all = torch.softmax(attention_scores_all, dim=-1)\n",
    "print(\"Attention weights matrix (each row sums to 1):\")\n",
    "print(attention_weights_all)\n",
    "print()\n",
    "\n",
    "# Verify that each row sums to 1\n",
    "print(\"Row sums (should all be 1.0):\")\n",
    "print(attention_weights_all.sum(dim=-1))\n",
    "print()\n",
    "\n",
    "# Compute context vectors for all words\n",
    "context_vectors_all = torch.matmul(attention_weights_all, inputs)\n",
    "print(\"Context vectors for all words:\")\n",
    "print(context_vectors_all)\n",
    "print()\n",
    "print(\"Shape:\", context_vectors_all.shape)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22148824",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Understanding the Results\n",
    "\n",
    "**Attention Scores Matrix (6x6):**\n",
    "- Each row `i` contains the raw attention scores for word `i` attending to all words\n",
    "- Higher values mean stronger attention\n",
    "\n",
    "**Attention Weights Matrix (6x6):**\n",
    "- Same as above but after softmax normalization\n",
    "- Each row sums to 1, making it a probability distribution\n",
    "- Row `i` shows how much word `i` attends to each word in the sequence\n",
    "\n",
    "**Context Vectors Matrix (6x3):**\n",
    "- Each row `i` is the context vector for word `i`\n",
    "- This is the contextualized representation of each word\n",
    "- Row 1 (index 1) should match our earlier single context vector for \"journey\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520dded0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that our single context vector for \"journey\" matches the matrix result\n",
    "print(\"Single context vector for 'journey' (computed earlier):\")\n",
    "print(context_vector)\n",
    "print()\n",
    "print(\"Context vector for 'journey' from matrix (row 1):\")\n",
    "print(context_vectors_all[1])\n",
    "print()\n",
    "print(\"Are they the same?\", torch.allclose(context_vector, context_vectors_all[1]))\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3b225888",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#implementing STANDARD self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7af9506c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3367,  0.1288,  0.2345],\n",
      "        [ 0.2303, -1.1229, -0.1863],\n",
      "        [ 2.2082, -0.6380,  0.4617]])\n",
      "tensor([[ 0.2674,  0.5349,  0.8094],\n",
      "        [ 1.1103, -1.6898, -0.9890],\n",
      "        [ 0.9580,  1.3221,  0.8172]])\n",
      "tensor([[-0.7658, -0.7506,  1.3525],\n",
      "        [ 0.6863, -0.3278,  0.7950],\n",
      "        [ 0.2815,  0.0562,  0.5227]])\n"
     ]
    }
   ],
   "source": [
    "# Randomly initializing the weight matrices\n",
    "torch.manual_seed(42)\n",
    "\n",
    "W_q = torch.randn(3, 3)\n",
    "W_k = torch.randn(3, 3)\n",
    "W_v = torch.randn(3, 3)\n",
    "\n",
    "print (W_q)\n",
    "print (W_k)\n",
    "print (W_v)\n",
    "\n",
    "# Computing the query, key, and value vectors\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "224421b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9192, 0.4008],\n",
      "        [0.9302, 0.6558],\n",
      "        [0.0766, 0.8460]])\n",
      "tensor([[0.3624, 0.3083],\n",
      "        [0.0850, 0.0029],\n",
      "        [0.6431, 0.3908]])\n",
      "tensor([[0.6947, 0.0897],\n",
      "        [0.8712, 0.1330],\n",
      "        [0.4137, 0.6044]])\n",
      "query vector for the word journey\n",
      "tensor([1.3654, 1.3493])\n",
      "key vector for the word journey\n",
      "tensor([[0.7409, 0.4808],\n",
      "        [0.6977, 0.4300],\n",
      "        [0.6904, 0.4283],\n",
      "        [0.3412, 0.1985],\n",
      "        [0.3646, 0.2772],\n",
      "        [0.4398, 0.2327]])\n",
      "value vector for the word journey\n",
      "tensor([[0.7975, 0.5965],\n",
      "        [1.4130, 0.5639],\n",
      "        [1.4012, 0.5510],\n",
      "        [0.7946, 0.2963],\n",
      "        [0.7941, 0.1627],\n",
      "        [0.9592, 0.4433]])\n"
     ]
    }
   ],
   "source": [
    "# First lets see how the context vector is computed for a single word\n",
    "\n",
    "x_2 = inputs[1] # word = journey\n",
    "d_in = inputs.shape[1] # dimension of the input vectors\n",
    "d_out = 2 # dimension of the output vectors\n",
    "\n",
    "W_q = torch.rand(d_in, d_out)\n",
    "W_k = torch.rand(d_in, d_out)\n",
    "W_v = torch.rand(d_in, d_out)\n",
    "\n",
    "print (W_q)\n",
    "print (W_k)\n",
    "print (W_v)\n",
    "\n",
    "q = x_2 @ W_q # query vector for the word \"journey\"\n",
    "k = inputs @ W_k # key vector for the word \"journey\"\n",
    "v = inputs @ W_v # value vector for the word \"journey\"\n",
    "\n",
    "print (\"query vector for the word journey\")\n",
    "print (q)\n",
    "print (\"key vector for the word journey\")\n",
    "print (k)\n",
    "print (\"value vector for the word journey\")\n",
    "print (v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7528d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Inputs\n",
      "tensor([[0.4300, 0.1500, 0.8900],\n",
      "        [0.5500, 0.8700, 0.6600],\n",
      "        [0.5700, 0.8500, 0.6400],\n",
      "        [0.2200, 0.5800, 0.3300],\n",
      "        [0.7700, 0.2500, 0.1000],\n",
      "        [0.0500, 0.8000, 0.5500]])\n",
      "Transpose of Inputs\n",
      "tensor([[0.4300, 0.5500, 0.5700, 0.2200, 0.7700, 0.0500],\n",
      "        [0.1500, 0.8700, 0.8500, 0.5800, 0.2500, 0.8000],\n",
      "        [0.8900, 0.6600, 0.6400, 0.3300, 0.1000, 0.5500]])\n",
      "Attention Weights\n",
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "Context Vector\n",
      "attention weights for all words in the sentence\n",
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "context vector for all words in the sentence\n",
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "# now calculating context vector for all words in the sentence\n",
    "\n",
    "print(\"Original Inputs\")\n",
    "print (inputs)\n",
    "\n",
    "print(\"Transpose of Inputs\")\n",
    "print (inputs.T)\n",
    "\n",
    "print(\"Attention Weights\")\n",
    "attention_weights = torch.softmax(torch.matmul(inputs, inputs.T), dim=-1) # attention weights for all words in the sentence\n",
    "print (attention_weights)\n",
    "\n",
    "print(\"Context Vector\")\n",
    "context_vector = torch.matmul(attention_weights, inputs) # context vector for all words in the sentence\n",
    "\n",
    "print (\"attention weights for all words in the sentence\")\n",
    "print (attention_weights)\n",
    "\n",
    "print (\"context vector for all words in the sentence\")\n",
    "print (context_vector)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad650882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
